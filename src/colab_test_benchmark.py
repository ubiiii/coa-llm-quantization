"""
Colab-compatible test script for benchmarking utilities.

This script can be run directly in Google Colab to test all benchmark functions.
"""

# Run this entire cell in Google Colab to test the benchmark utilities

print("ğŸ§ª Testing Benchmarking Utilities in Colab")
print("=" * 50)

try:
    # Import required libraries
    from benchmark import LLMBenchmark, quick_benchmark
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    
    print("âœ… All imports successful")
    
    # Load a small model for testing
    print("\nğŸ“¥ Loading test model...")
    model_name = "microsoft/DialoGPT-small"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    print("âœ… Model loaded successfully")
    
    # Test 1: LLMBenchmark initialization
    print("\nğŸ”§ Test 1: LLMBenchmark initialization...")
    benchmark = LLMBenchmark(model, tokenizer)
    print("âœ… LLMBenchmark initialized successfully")
    
    # Test 2: Hardware utilization
    print("\nğŸ–¥ï¸ Test 2: Hardware utilization measurement...")
    hardware_results = benchmark.measure_hardware_utilization()
    print(f"âœ… Hardware: {hardware_results['gpu_name'] if hardware_results.get('gpu_available') else 'CPU'}")
    
    # Test 3: Output quality
    print("\nğŸ“ Test 3: Output quality measurement...")
    quality_results = benchmark.measure_output_quality("Hello, how are you?")
    print(f"âœ… Quality score: {quality_results['quality_score']}/5")
    print(f"âœ… Generated: '{quality_results['generated_text']}'")
    
    # Test 4: Memory usage
    print("\nğŸ’¾ Test 4: Memory usage measurement...")
    memory_results = benchmark.measure_memory_usage("Hello, how are you?")
    print(f"âœ… Peak memory: {memory_results['peak_memory_gb']:.2f} GB")
    
    # Test 5: Inference speed (quick test)
    print("\nâš¡ Test 5: Inference speed measurement...")
    speed_results = benchmark.measure_inference_speed("Hello, how are you?", num_runs=5, warmup_runs=2)
    print(f"âœ… Speed: {speed_results['tokens_per_second']:.2f} tokens/sec")
    
    # Test 6: Quick benchmark function
    print("\nğŸš€ Test 6: Quick benchmark function...")
    quick_results = quick_benchmark(model, tokenizer, "Hello, how are you?")
    print(f"âœ… Quick benchmark: {quick_results['tokens_per_second']:.2f} tokens/sec")
    
    print("\nğŸ‰ ALL TESTS PASSED!")
    print("=" * 50)
    print("âœ… Benchmark utilities are fully functional in Colab!")
    print("âœ… Ready for Phase 3 experiments!")
    
except Exception as e:
    print(f"âŒ Test failed with error: {e}")
    print("Please check that benchmark.py is uploaded and accessible")
