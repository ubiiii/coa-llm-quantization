# COA Project: Hardware/Software Co-Design for LLM Quantization
# Requirements file for reproducible environment setup
# Team: CipherCore (Utkarsh & Sami)
# Date: January 19, 2025

# Core ML Libraries
torch==2.8.0+cu126
torchvision==0.17.0+cu126
torchaudio==2.8.0+cu126
transformers==4.44.2
tokenizers==0.19.1
accelerate==1.10.1

# Quantization Libraries
bitsandbytes==0.48.1
auto-gptq==0.6.0
autoawq==0.2.3

# ONNX Runtime (for Task 3.9)
onnx==1.19.1
onnxruntime==1.23.1
onnxscript==0.5.4

# Data Processing
numpy==1.24.3
pandas==2.0.3
datasets==2.14.5
tokenizers==0.19.1

# Visualization and Analysis
matplotlib==3.7.2
seaborn==0.12.2
plotly==5.15.0

# Utilities
tqdm==4.65.0
psutil==5.9.5
GPUtil==1.4.0

# Jupyter and Development
jupyter==1.0.0
ipywidgets==8.0.7
notebook==6.5.4

# Testing and Quality
pytest==7.4.0
black==23.7.0
flake8==6.0.0

# Documentation
sphinx==7.1.2
sphinx-rtd-theme==1.3.0

# Google Colab Specific (if using Colab)
google-colab==1.0.0

# CUDA Support (for local development)
# Note: Install appropriate CUDA toolkit version based on your system
# CUDA 12.6 recommended for compatibility with PyTorch 2.8.0

# Additional Dependencies for Advanced Features
# Uncomment if needed:
# vllm==0.2.5  # For advanced inference optimization
# flash-attn==2.3.4  # For memory-efficient attention
# xformers==0.0.22  # For optimized transformers

# Version Notes:
# - PyTorch 2.8.0: Latest stable version with CUDA 12.6 support
# - Transformers 4.44.2: Compatible with all tested models
# - ONNX Runtime 1.23.1: Latest version with full INT8 support
# - BitsAndBytes 0.48.1: Latest version with improved quantization