model_name,quantization_method,precision,model_size_mb,parameters_millions,inference_speed_tokens_per_sec,memory_usage_gb,memory_reduction_percent,speedup_factor,accuracy_metric,hardware_config,gpu_utilization_percent,tensor_core_usage,test_prompt,generated_output,date,timestamp,experiment_purpose,summary,notes
microsoft/DialoGPT-small,None,FP16,351.0,124.4,28.42,0.54,0.0,1.0,3/5,Tesla_T4_CUDA_12.6,45.2,No,"Hello, how are you?","Hello, how are you? Good morning everyone!",2025-10-10,01:15:00,BASELINE_ESTABLISHMENT,"Comprehensive 100-run baseline benchmark to establish reliable performance baseline for quantization comparison","Final baseline measurement using LLMBenchmark class with 100 runs + 10 warmup runs. This serves as the reference point for all subsequent quantization experiments. Statistical reliability: Â±0.055s standard deviation."
distilgpt2,ONNX_Runtime,FP32,460.95,82.0,14.4,0.69,0.0,1.0,3/5,CPU_Only,0,No,"The quick brown fox","es, the black-and-white, and the black-and-white, and the black",2025-01-19,15:50:00,TASK_3_9_BASELINE,"ONNX Runtime baseline performance for distilgpt2 model with standard inference","Professional ONNX export implementation with proper dynamic axes and I/O naming for production deployment"
distilgpt2,ONNX_Runtime_INT8,INT8,229.14,82.0,24.4,0.35,50.3,1.69,3/5,CPU_Only,0,No,"The quick brown fox","es, the black-and-white, and the black-and-white, and the black",2025-01-19,15:51:00,TASK_3_9_QUANTIZATION,"ONNX Runtime INT8 quantized performance showing significant speedup and memory reduction","Dynamic INT8 quantization with 50% size reduction and 1.69x speedup. Demonstrates effective hardware-assisted inference optimization"
distilgpt2,ONNX_Runtime_KV_Cache,FP32,460.95,82.0,98.3,0.69,0.0,6.8,3/5,CPU_Only,0,No,"The quick brown fox","es, the black-and-white, and the black-and-white, and the black",2025-01-19,15:52:00,TASK_3_9_AUTOREGRESSIVE,"ONNX Runtime with KV cache for efficient autoregressive generation","Advanced ONNX export with KV cache support enabling efficient autoregressive generation at 10.17ms per token"
distilgpt2,ONNX_Runtime_KV_Cache_INT8,INT8,229.14,82.0,98.3,0.35,50.3,6.8,3/5,CPU_Only,0,No,"The quick brown fox","es, the black-and-white, and the black-and-white, and the black",2025-01-19,15:53:00,TASK_3_9_COMPLETE,"Complete ONNX Runtime solution with KV cache and INT8 quantization","Professional-grade implementation combining autoregressive generation with quantization optimization"
distilgpt2,BitsAndBytes_INT8,FP16,460.95,82.0,91.81,0.35,0.0,1.0,3/5,Tesla_T4_CUDA_12.6,15.0,No,"Hello, how are you?","Hello, how are you?\n\n\n\n\n\nI can't remember anything for sure.\n\nThe next time you write an article, I'll be sure to ask",2025-01-19,16:00:00,ACCURACY_TESTING,"Comprehensive accuracy testing with WikiText-2 perplexity measurement","FP16 baseline for accuracy comparison with perplexity: 82.28, avg_loss: 4.41, tokens: 6503"
distilgpt2,BitsAndBytes_INT8,INT8,229.14,82.0,59.93,0.31,12.0,0.65,3/5,Tesla_T4_CUDA_12.6,14.0,No,"Hello, how are you?","Hello, how are you?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",2025-01-19,16:01:00,ACCURACY_TESTING,"Comprehensive accuracy testing with WikiText-2 perplexity measurement","INT8 quantized accuracy comparison with perplexity: 83.20, avg_loss: 4.42, tokens: 6503, degradation: 1.12%"
microsoft/DialoGPT-small,BitsAndBytes_INT8,FP16,351.0,124.4,28.42,0.54,0.0,1.0,3/5,Tesla_T4_CUDA_12.6,45.2,No,"Hello, how are you?","Hello, how are you? After the episode, I was disappointed.",2025-01-19,16:02:00,ACCURACY_TESTING,"Comprehensive accuracy testing with WikiText-2 perplexity measurement","FP16 baseline for accuracy comparison with perplexity: 41021.00, avg_loss: 10.62, tokens: 6503"
microsoft/DialoGPT-small,BitsAndBytes_INT8,INT8,175.5,124.4,5.58,0.27,50.0,0.52,3/5,Tesla_T4_CUDA_12.6,45.2,No,"Hello, how are you?","Hello, how are you? Good morning everyone. How are you today?",2025-01-19,16:03:00,ACCURACY_TESTING,"Comprehensive accuracy testing with WikiText-2 perplexity measurement","INT8 quantized accuracy comparison with perplexity: 42375.57, avg_loss: 10.65, tokens: 6503, degradation: 3.30%"
