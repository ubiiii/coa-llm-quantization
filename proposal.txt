Team - CipherCore
Proposal 1: Hardware/Software Co-Design for LLM Quantization (Project 12)
Background:
Large Language Models (LLMs) require enormous compute and memory resources during
inference. Quantization (reducing precision from FP32 to INT8/4) is one of the most widely used
techniques for improving efficiency, but software-only approaches often suffer from accuracy
drop and limited hardware utilization. Hardware/software co-design can bridge this gap by
aligning quantization methods with specialized instructions and microarchitectural support.
Goal:
Investigate hardware-aware quantization strategies for LLM inference and compare
performance/accuracy trade-offs between hardware-assisted and software-only quantization
approaches.
Expected Deliverables:
● Survey of HW/SW co-design methods for LLM quantization.
● Experimental results using quantization libraries (e.g., BitsandBytes) on small models.
● Analysis of performance, memory footprint, and accuracy trade-offs.
Purpose / Why I Chose This Topic:
Utkarsh: My long-term goal is to build a career and eventually a company in the AI infrastructure space.
LLMs are at the heart of today’s AI revolution, but their efficiency is the main bottleneck. By exploring
HW/SW co-design for quantization, I can develop cross-stack thinking, not just software, not just
hardware, but how both interact to enable real-world deployment. This aligns with my personal ambition
to become a systems thinker who can design solutions that range from research to industry.
Sami: - I chose this topic because I want to understand how hardware and software work together to
make LLMs more efficient. Quantization interests me since it’s a practical way to reduce cost and speed
up inference without losing too much accuracy. Working on this project will help me build the kind of
cross-disciplinary skills I need as a future data scientist, where efficiency and scalability are just as
important as building the models themselves.
References:
● Tim Dettmers et al., BitsandBytes: 8-bit Optimizers & Quantization
(https://github.com/TimDettmers/bitsandbytes)
● ISCA/MICRO papers on quantization-aware accelerators (2019–2024).